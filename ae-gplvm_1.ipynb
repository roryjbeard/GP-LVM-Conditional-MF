{
 "metadata": {
  "name": "",
  "signature": "sha256:b4f98d5e11ea959e8ca8854d11e63924d414662a8145a4f849024e2056f67db4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from theano.tensor.shared_randomstreams import RandomStreams\n",
      "from printable import Printable\n",
      "from jitterProtect import JitterProtect\n",
      "from nnet import MLP_Network\n",
      "import lasagne\n",
      "from parmesan.distributions import log_stdnormal, log_normal2, log_normal, kl_normal1_stdnormal\n",
      "from parmesan.layers import NormalizeLayer, ScaleAndShiftLayer, ListIndexLayer\n",
      "from parmesan.datasets import load_mnist_realval, load_omniglot, load_omniglot_iwae, load_norb_small, load_mnist_binarized\n",
      "from GP_LVM_CMF import kernelFactory\n",
      "import shutil, gzip, os, cPickle, time, operator, argparse\n",
      "\n",
      "from utils import cholInvLogDet, sharedZeroMatrix, sharedZero3Tensor, \\\n",
      "    dot, minus, plus, div, conditionNumber\n",
      "\n",
      "theano.config.floatX = 'float32'\n",
      "precision = theano.config.floatX\n",
      "log2pi = T.constant(np.log(2 * np.pi))\n",
      "jitterProtect = JitterProtect(precision)\n",
      "\n",
      "srng = RandomStreams(seed=234)\n",
      "nan_noise = T.constant(1e-6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jitterProtect.jitter.type"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 132,
       "text": [
        "TensorType(float32, scalar)"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyticalPhiStats = False\n",
      "enc_type = 'MLP_diag'\n",
      "batch_norm = 'True'\n",
      "dataset = 'mnistresample'\n",
      "batch_size = 256\n",
      "batch_size_test = 25\n",
      "dimX = 5\n",
      "n_induce = 50\n",
      "kernelType = 'ARD'\n",
      "hidden_sizes = [100]\n",
      "nonlin_enc = 'leaky_rectify'\n",
      "num_mlp_layers = 1\n",
      "num_epochs = 5\n",
      "lr = 0.002\n",
      "verbose = True\n",
      "\n",
      "res_out = 'gplvm_out'\n",
      "\n",
      "def verbose_print(text):\n",
      "    if verbose: print text\n",
      "        \n",
      "if not os.path.exists(res_out):\n",
      "    os.makedirs(res_out)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##### some stuff copied from GPy #####\n",
      "# Psi statistics computations for RBF kernel\n",
      "\n",
      "def psicomputations(variance, lengthscale, Z, X_mean, X_var, return_psi2_n=False):\n",
      "    # here are the \"statistics\" for psi0, psi1 and psi2\n",
      "    # Produced intermediate results:\n",
      "    # _psi1                NxM\n",
      "    mu = X_mean\n",
      "    S = X_var\n",
      "\n",
      "    psi0 = np.empty(mu.shape[0])\n",
      "    psi0[:] = variance\n",
      "    psi1 = _psi1computations(variance, lengthscale, Z, mu, S)\n",
      "    psi2 = _psi2computations(variance, lengthscale, Z, mu, S)\n",
      "    if not return_psi2_n: psi2 = psi2.sum(axis=0)\n",
      "    return psi0, psi1, psi2\n",
      "\n",
      "\n",
      "def __psi1computations(variance, lengthscale, Z, mu, S):\n",
      "    # here are the \"statistics\" for psi1\n",
      "    # Produced intermediate results:\n",
      "    # _psi1                NxM\n",
      "\n",
      "    lengthscale2 = T.square(lengthscale)\n",
      "\n",
      "    # psi1\n",
      "    _psi1_logdenom = T.log(S/lengthscale2+1.).sum(axis=-1) # N\n",
      "    # _psi1_log = (_psi1_logdenom[:,None]+np.einsum('nmq,nq->nm',np.square(mu[:,None,:]-Z[None,:,:]),1./(S+lengthscale2)))/(-2.)\n",
      "    _psi1_log = (_psi1_logdenom[:,None]+T.batched_tensorprod(T.square(mu[:,None,:]-Z[None,:,:]),1./(S+lengthscale2), axes=[[2],[1]]))/(-2.)\n",
      "    _psi1 = variance*T.exp(_psi1_log)\n",
      "\n",
      "    return _psi1\n",
      "\n",
      "def __psi2computations(variance, lengthscale, Z, mu, S):\n",
      "    # here are the \"statistics\" for psi2\n",
      "    # Produced intermediate results:\n",
      "    # _psi2                MxM\n",
      "\n",
      "    N,M,Q = mu.shape[0], Z.shape[0], mu.shape[1]\n",
      "    lengthscale2 = T.square(lengthscale)\n",
      "\n",
      "    _psi2_logdenom = T.log(2.*S/lengthscale2+1.).sum(axis=-1)/(-2.) # N\n",
      "    _psi2_exp1 = (T.square(Z[:,None,:]-Z[None,:,:])/lengthscale2).sum(axis=-1)/(-4.) #MxM\n",
      "    Z_hat = (Z[:,None,:]+Z[None,:,:])/2. #MxMxQ\n",
      "    denom = 1./(2.*S+lengthscale2)\n",
      "    _psi2_exp2 = -(T.square(mu)*denom).sum(axis=-1)[:,None,None]+(2*(mu*denom).dot(Z_hat.reshape(M*M,Q).T) - denom.dot(T.square(Z_hat).reshape(M*M,Q).T)).reshape(N,M,M)\n",
      "    _psi2 = variance*variance*T.exp(_psi2_logdenom[:,None,None]+_psi2_exp1[None,:,:]+_psi2_exp2)\n",
      "    return _psi2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "desc = \"\"\n",
      "test_t = None\n",
      "\n",
      "def bernoullisample(x):\n",
      "    return np.random.binomial(1,x,size=x.shape).astype(theano.config.floatX)\n",
      "\n",
      "#load dataset\n",
      "regularize_var = False\n",
      "if dataset == 'mnistfixedbin':\n",
      "    drawsamples = True\n",
      "    print \"Using fixed binarised mnist dataset\"\n",
      "    process_data = lambda y: y\n",
      "    train_y, valid_y, test_y = load_mnist_binarized()\n",
      "    train_y = np.concatenate([train_y,valid_y])\n",
      "    idx = np.random.permutation(test_y.shape[0])\n",
      "    test_y = test_y[idx]\n",
      "    pcaplot = True\n",
      "    num_class = 10\n",
      "    h,w = 28,28\n",
      "    ntrain = train_y.shape[0]\n",
      "    ntest = train_y.shape[0]\n",
      "    num_features = h*w\n",
      "    outputdensity = 'bernoulli'\n",
      "    outputnonlin = lasagne.nonlinearities.sigmoid\n",
      "    imgshp = [h,w]\n",
      "elif dataset == 'mnistresample':\n",
      "    drawsamples = True\n",
      "    print \"Using resampled mnist dataset\"\n",
      "    process_data = bernoullisample\n",
      "    train_y, train_t, valid_y, valid_t, test_y, test_t = load_mnist_realval()\n",
      "    train_y = np.concatenate([train_y,valid_y])\n",
      "    test_y = process_data(test_y)\n",
      "    idx = np.random.permutation(test_y.shape[0])\n",
      "    test_y = test_y[idx]\n",
      "    test_t = test_t[idx]\n",
      "    pcaplot = True\n",
      "    num_class = 10\n",
      "    h,w = 28,28\n",
      "    ntrain = train_y.shape[0]\n",
      "    ntest = train_y.shape[0]\n",
      "    num_features = h*w\n",
      "    outputdensity = 'bernoulli'\n",
      "    outputnonlin = lasagne.nonlinearities.sigmoid\n",
      "    imgshp = [h,w]\n",
      "elif dataset == 'omniglot':\n",
      "    drawsamples = True\n",
      "    print \"Using omniglot dataset\"\n",
      "    train_y, test_y = load_omniglot()\n",
      "    np.random.shuffle(train_y)\n",
      "    np.random.shuffle(test_y)\n",
      "    process_data = bernoullisample\n",
      "    h,w = 32,32\n",
      "    pcaplot = True\n",
      "    ntrain = train_y.shape[0]\n",
      "    ntest = test_y.shape[0]\n",
      "    num_features = h*w\n",
      "    train_y = train_y.reshape(-1,num_features)\n",
      "    test_y = test_y.reshape(-1,num_features)\n",
      "    outputdensity = 'bernoulli'\n",
      "    outputnonlin = lasagne.nonlinearities.sigmoid\n",
      "    imgshp = [h,w]\n",
      "elif dataset == 'omniglot_iwae':\n",
      "    drawsamples = True\n",
      "    print \"Using omniglot dataset\"\n",
      "    train_y, train_t, train_char, test_y, test_t, test_char = load_omniglot_iwae()\n",
      "    np.random.shuffle(train_y)\n",
      "    np.random.shuffle(test_y)\n",
      "    process_data = bernoullisample\n",
      "    num_class = 50\n",
      "    h,w = 28,28\n",
      "    pcaplot = True\n",
      "    ntrain = train_y.shape[0]\n",
      "    ntest = test_y.shape[0]\n",
      "    num_features = h*w\n",
      "    train_y = train_y.reshape(-1,num_features)\n",
      "    test_y = test_y.reshape(-1,num_features)\n",
      "    outputdensity = 'bernoulli'\n",
      "    outputnonlin = lasagne.nonlinearities.sigmoid\n",
      "    imgshp = [h,w]\n",
      "elif dataset == 'norb_small':\n",
      "    print \"Using norb_small dataset\"\n",
      "    process_data = lambda y: y\n",
      "    train_y, train_t, test_y, test_t = load_norb_small(normalize=True,dequantify=True)\n",
      "    ntrain = train_y.shape[0]\n",
      "    ntest = train_y.shape[0]\n",
      "    h,w = 32,32\n",
      "    num_features = h*w\n",
      "    pcaplot = True\n",
      "    num_class = 5\n",
      "    outputdensity = 'gaussian'\n",
      "    outputnonlin = lasagne.nonlinearities.linear\n",
      "    imgshp = [h,w]\n",
      "    drawsamples = True\n",
      "else:\n",
      "    raise ValueError()\n",
      "    \n",
      "def get_nonlin(nonlin):\n",
      "    if nonlin == 'rectify':\n",
      "        return lasagne.nonlinearities.rectify\n",
      "    elif nonlin == 'leaky_rectify':\n",
      "        return lasagne.nonlinearities.leaky_rectify\n",
      "    elif nonlin == 'very_leaky_rectify':\n",
      "        return lasagne.nonlinearities.very_leaky_rectify\n",
      "    elif nonlin == 'tanh':\n",
      "        return lasagne.nonlinearities.tanh\n",
      "    elif nonlin == 'capped_leaky_rectify':\n",
      "        return lambda x: T.clip(lasagne.nonlinearities.leaky_rectify(x),-0.01*2,2)\n",
      "    else:\n",
      "        raise ValueError()\n",
      "    \n",
      "nonlin_enc = get_nonlin(nonlin_enc)    \n",
      "\n",
      "B = batch_size\n",
      "P = train_y.shape[1]\n",
      "R = dimX\n",
      "M = n_induce"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using resampled mnist dataset\n"
       ]
      }
     ],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Parmesan stuff for MLP encoder\n",
      "class SampleLayer(lasagne.layers.MergeLayer):\n",
      "    \"\"\"\n",
      "    Sampling layer supporting importance sampling as described in [BURDA]_ and\n",
      "    multiple Monte Carlo samples for the approximation of\n",
      "    E_q [log( p(x,z) / q(z|x) )].\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    mu, log_var : :class:`Layer` instances\n",
      "        Parameterizing the mean and log(variance) of the distribution to sample\n",
      "        from as described in [BURDA]_. The code assumes that these have the same\n",
      "        number of dimensions.\n",
      "\n",
      "    eq_samples : int or T.scalar\n",
      "        Number of Monte Carlo samples used to estimate the expectation over\n",
      "        q(z|x) in eq. (8) in [BURDA]_.\n",
      "\n",
      "    iw_samples : int or T.scalar\n",
      "        Number of importance samples in the sum over k in eq. (8) in [BURDA]_.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "        ..  [BURDA] Burda, Yuri, Roger Grosse, and Ruslan Salakhutdinov.\n",
      "            \"Importance Weighted Autoencoders.\"\n",
      "            arXiv preprint arXiv:1509.00519 (2015).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, mu, var, eq_samples=1, iw_samples=1, **kwargs):\n",
      "        super(SampleLayer, self).__init__([mu, var], **kwargs)\n",
      "\n",
      "        self.eq_samples = eq_samples\n",
      "        self.iw_samples = iw_samples\n",
      "\n",
      "        self._srng = RandomStreams(\n",
      "            lasagne.random.get_rng().randint(1, 2147462579))\n",
      "\n",
      "    def get_output_shape_for(self, input_shapes):\n",
      "        batch_size, num_latent = input_shapes[0]\n",
      "        if isinstance(batch_size, int) and \\\n",
      "           isinstance(self.iw_samples, int) and \\\n",
      "           isinstance(self.eq_samples, int):\n",
      "            out_dim = (batch_size*self.eq_samples*self.iw_samples, num_latent)\n",
      "        else:\n",
      "            out_dim = (None, num_latent)\n",
      "        return out_dim\n",
      "\n",
      "    def get_output_for(self, input, **kwargs):\n",
      "        mu, var = input\n",
      "        batch_size, num_latent = mu.shape\n",
      "        eps = self._srng.normal(\n",
      "            [batch_size, self.eq_samples, self.iw_samples, num_latent],\n",
      "             dtype=theano.config.floatX)\n",
      "\n",
      "        z = mu.dimshuffle(0,'x','x',1) + \\\n",
      "                T.sqrt(var).dimshuffle(0,'x','x',1) * eps\n",
      "\n",
      "        return z.reshape((-1,num_latent))\n",
      "    \n",
      "    \n",
      "class SampleLayer_chol(lasagne.layers.MergeLayer):\n",
      "    \"\"\"\n",
      "    Sampling layer supporting importance sampling as described in [BURDA]_ and\n",
      "    multiple Monte Carlo samples for the approximation of\n",
      "    E_q [log( p(x,z) / q(z|x) )].\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    mu, var_chol : :class:`Layer` instances\n",
      "        Parameterizing the mean and log(variance) of the distribution to sample\n",
      "        from as described in [BURDA]_. The code assumes that these have the same\n",
      "        number of dimensions.\n",
      "\n",
      "    eq_samples : int or T.scalar\n",
      "        Number of Monte Carlo samples used to estimate the expectation over\n",
      "        q(z|x) in eq. (8) in [BURDA]_.\n",
      "\n",
      "    iw_samples : int or T.scalar\n",
      "        Number of importance samples in the sum over k in eq. (8) in [BURDA]_.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "        ..  [BURDA] Burda, Yuri, Roger Grosse, and Ruslan Salakhutdinov.\n",
      "            \"Importance Weighted Autoencoders.\"\n",
      "            arXiv preprint arXiv:1509.00519 (2015).\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, mu, var_chol, eq_samples=1, iw_samples=1, **kwargs):\n",
      "        super(SampleLayer_chol, self).__init__([mu, var_chol], **kwargs)\n",
      "\n",
      "        self.eq_samples = eq_samples\n",
      "        self.iw_samples = iw_samples\n",
      "\n",
      "        self._srng = RandomStreams(\n",
      "            lasagne.random.get_rng().randint(1, 2147462579))\n",
      "\n",
      "    def get_output_shape_for(self, input_shapes):\n",
      "        batch_size, num_latent = input_shapes[0]\n",
      "        if isinstance(batch_size, int) and \\\n",
      "           isinstance(self.iw_samples, int) and \\\n",
      "           isinstance(self.eq_samples, int):\n",
      "            out_dim = (batch_size*self.eq_samples*self.iw_samples, num_latent)\n",
      "        else:\n",
      "            out_dim = (None, num_latent)\n",
      "        return out_dim\n",
      "\n",
      "    def get_output_for(self, input, **kwargs):\n",
      "        mu, var_chol = input\n",
      "        batch_size, num_latent = mu.shape\n",
      "        eps = self._srng.normal(\n",
      "            [batch_size, self.eq_samples, self.iw_samples, num_latent],\n",
      "             dtype=theano.config.floatX)\n",
      "\n",
      "        z = mu.dimshuffle(0,'x','x',1) + \\\n",
      "                T.dot(var_chol.dimshuffle(0,'x','x',1), eps)\n",
      "\n",
      "        return z.reshape((-1,num_latent))\n",
      "\n",
      "\n",
      "num_layers = 1\n",
      "\n",
      "w_init_mu = lasagne.init.GlorotNormal(1.0)\n",
      "b_init_var = lasagne.init.Constant(1.0)\n",
      "w_init_var = lasagne.init.GlorotNormal(1.0)\n",
      "w_init_sigmoid = lasagne.init.GlorotNormal(1.0)\n",
      "w_init_mlp = lasagne.init.GlorotNormal('relu')\n",
      "\n",
      "sym_iw_samples = T.iscalar('iw_samples')\n",
      "sym_eq_samples = T.iscalar('eq_samples')\n",
      "sym_lr = T.scalar('lr')\n",
      "sym_y = T.matrix()\n",
      "\n",
      "test_y = process_data(test_y)\n",
      "Y = process_data(train_y)[:batch_size]\n",
      "\n",
      "def get_mu_var(inputs):\n",
      "    mu, var = ListIndexLayer(inputs,index=0),ListIndexLayer(inputs,index=1)\n",
      "    return mu, var\n",
      "\n",
      "\n",
      "def batchnormlayer(l,num_units, nonlinearity, name, W=lasagne.init.GlorotUniform(), b=lasagne.init.Constant(0.)):\n",
      "    l = lasagne.layers.DenseLayer(l, num_units=num_units, name=\"Dense-\" + name, W=W, b=b, nonlinearity=None)\n",
      "    l = NormalizeLayer(l,name=\"BN-\" + name)\n",
      "    l = ScaleAndShiftLayer(l,name=\"SaS-\" + name)\n",
      "    l = lasagne.layers.NonlinearityLayer(l,nonlinearity=nonlinearity,name=\"Nonlin-\" + name)\n",
      "    return l\n",
      "\n",
      "def normaldenselayer(l,num_units, nonlinearity, name, W=lasagne.init.GlorotUniform(), b=lasagne.init.Constant(0.)):\n",
      "    l = lasagne.layers.DenseLayer(l, num_units=num_units, name=\"Dense-\" + name, W=W, b=b, nonlinearity=nonlinearity)\n",
      "    return l\n",
      "\n",
      "if batch_norm:\n",
      "    print \"Using batch Normalization - The current implementation calculates \" \\\n",
      "          \"the BN constants on the complete dataset in one batch. This might \" \\\n",
      "          \"cause memory problems on some GFX's\"\n",
      "    denselayer = batchnormlayer\n",
      "else:\n",
      "    denselayer = normaldenselayer\n",
      "\n",
      "\n",
      "def mlp(l,num_units, nonlinearity, name, num_mlp_layers=1, W=lasagne.init.GlorotUniform(), b=lasagne.init.Constant(0.),):\n",
      "    outputlayer = l\n",
      "    for i in range(num_mlp_layers):\n",
      "        outputlayer = denselayer(outputlayer, num_units=num_units, name=name+'_'+str(i+1), nonlinearity=nonlinearity, W=W, b=b)\n",
      "    return outputlayer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Using batch Normalization - The current implementation calculates the BN constants on the complete dataset in one batch. This might cause memory problems on some GFX's\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# kernelType = params['kernelType']\n",
      "\n",
      "if kernelType == 'RBF':\n",
      "    numberOfKernelParameters = 2\n",
      "elif kernelType == 'RBFnn':\n",
      "    numberOfKernelParameters = 1\n",
      "elif kernelType == 'ARD':\n",
      "    numberOfKernelParameters = R + 1\n",
      "else:\n",
      "    raise RuntimeError('Unrecognised kernel type')\n",
      "\n",
      "kfactory = kernelFactory(kernelType)\n",
      "\n",
      "# kernel parameters of Kuu, Kuf, Kff\n",
      "log_theta = sharedZeroMatrix(1, numberOfKernelParameters,\n",
      "                                  '_log_theta',\n",
      "                                  dtype = precision,\n",
      "                                  broadcastable=(True, False))\n",
      "log_sigma_y = theano.shared(np.asarray(0, dtype=precision), name='_log_sigma_y')\n",
      "sigma_y = T.exp(log_sigma_y)\n",
      "\n",
      "gradientVariables = [log_theta, log_sigma_y]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Random variables\n",
      "alpha = srng.normal(size=(B, R), avg=0.0, std=1.0, ndim=None)\n",
      "beta = srng.normal(size=(B, P), avg=0.0, std=1.0, ndim=None)\n",
      "gamma = srng.normal(size=(M, P), avg=0.0, std=1.0, ndim=None)\n",
      "\n",
      "alpha.name = 'alpha'\n",
      "beta.name = 'beta'\n",
      "gamma.name = 'gamma'\n",
      "\n",
      "# COME BACK TO THIS... it might lead to a state update side effect\n",
      "sample_alpha = theano.function([], alpha)\n",
      "sample_beta = theano.function([], beta)\n",
      "sample_gamma = theano.function([], gamma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 138
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Variational distribution over latent coordinates\n",
      "# parameterised either mean field or via some form of recog net (encoder)\n",
      "\n",
      "if enc_type == 'None':\n",
      "    # mean-field\n",
      "    m_X = sharedZeroMatrix(B, R, '_m_X', dtype=precision)\n",
      "    log_S_X = sharedZeroMatrix(B, R, '_log_S_X', dtype=precision) #sdevs for mean-field\n",
      "    gradientVariables.extend([m_X, log_S_X])\n",
      "    S_X = T.exp(log_S_X)\n",
      "    mu_qXf = m_X\n",
      "    log_sigma_qXf = S_X\n",
      "    # Calculate latent co-ordinates Xf\n",
      "    L_qX_alpha = T.exp(log_sigma_qXf).T * alpha\n",
      "    # [BxR]  = [BxR] + [BxB] . [BxR]\n",
      "    Xf = mu_qXf.T + L_qX_alpha\n",
      "    Xf_get_value = theano.function([], Xf, no_default_updates=True)\n",
      "elif enc_type.startswith('MLP'):\n",
      "#     mlp_qX = MLP_Network(P, R, 'qX',\n",
      "#                               num_units=num_units,\n",
      "#                               num_layers=num_layers)\n",
      "#     mu_qX, log_sigma_qX = mlp_qX.setup(y_miniBatch.T)\n",
      "\n",
      "    if enc_type == 'MLP_diag':\n",
      "        l_in = lasagne.layers.InputLayer((None, P))\n",
      "        l_enc_h = mlp(l_in, num_units=hidden_sizes[0], W=w_init_mlp, name='ENC_A_DENSE%i'%0, nonlinearity=nonlin_enc, num_mlp_layers=num_mlp_layers)\n",
      "        lenc_Xf_mu = denselayer(l_enc_h, num_units=R, W=w_init_mu, nonlinearity=lasagne.nonlinearities.identity, name='ENC_A_MU%i'%0)\n",
      "        lenc_Xf_var = denselayer(l_enc_h, num_units=R, W=w_init_var, nonlinearity=lasagne.nonlinearities.softplus, b=b_init_var, name='ENC_A_var%i'%0)\n",
      "        l_Xf = SampleLayer(mu=lenc_Xf_mu, var=lenc_Xf_var)\n",
      "    elif enc_teyp == 'MLP_chol':\n",
      "        # INCOMPLETE!!!!!!!!!!!!\n",
      "        l_in = lasagne.layers.InputLayer((None, P))\n",
      "        l_enc_h_Xf_mu = mlp(l_in, num_units=hidden_sizes[0], W=w_init_mlp, name='ENC_A_DENSE_Xf_mu%i'%0, nonlinearity=nonlin_enc, num_mlp_layers=num_mlp_layers)\n",
      "        l_enc_h_Xf_chol = mlp(l_in, num_units=hidden_sizes[0], W=w_init_mlp, name='ENC_A_DENSE_Xf_chol%i'%0, nonlinearity=nonlin_enc, num_mlp_layers=num_mlp_layers)\n",
      "        lenc_Xf_mu = denselayer(l_enc_h_Xf_mu, num_units=R, W=w_init_mu, nonlinearity=lasagne.nonlinearities.identity, name='ENC_A_MU%i'%0)\n",
      "        lenc_Xf_var = denselayer(l_enc_h, num_units=(R**2+R)/2, W=w_init_var, nonlinearity=lasagne.nonlinearities.softplus, b=b_init_var, name='ENC_A_var%i'%0)\n",
      "        l_Xf = SampleLayer_chol(mu=lenc_Xf_mu, var_chol=lenc_Xf_var)\n",
      "\n",
      "    gradientVariables.extend(lasagne.layers.get_all_params(l_Xf, trainable=True))\n",
      "\n",
      "    # get output needed for evaluating model with noise if present\n",
      "    train_layers = lasagne.layers.get_output([l_Xf, lenc_Xf_mu, lenc_Xf_var], sym_y, deterministic=False)\n",
      "    Xf_train = train_layers[:1][0]\n",
      "    mu_qXf_train = train_layers[1:2][0]\n",
      "    var_qXf_train = train_layers[2:3][0]\n",
      "    sigma_qXf_train = T.sqrt(var_qXf_train + nan_noise)\n",
      "\n",
      "    test_layers = lasagne.layers.get_output([l_Xf, lenc_Xf_mu, lenc_Xf_var], {l_in:sym_y}, deterministic=True)\n",
      "    Xf_test = test_layers[:1][0]\n",
      "    mu_qXf_test = test_layers[1:2][0]\n",
      "    var_qXf_test = test_layers[2:3][0]\n",
      "    sigma_qXf_test = T.sqrt(var_qXf_test + nan_noise)\n",
      "    \n",
      "#     xf = lasagne.layers.get_output([l_Xf], sym_y, deterministic=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 139
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "func = theano.function([sym_y], mu_qXf)\n",
      "func(Y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'mu_qXf' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-140-adb93edb2f02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msym_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_qXf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'mu_qXf' is not defined"
       ]
      }
     ],
     "prompt_number": 140
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Inducing points co-ordinates\n",
      "Xu = sharedZeroMatrix(M, R, '_Xu', dtype=precision)\n",
      "gradientVariables.extend([Xu])\n",
      "\n",
      "# Kernels\n",
      "Kff_train = kfactory.kernel(Xf_train, None,    log_theta, 'Kff_train', dtype=precision)\n",
      "Kff_test = kfactory.kernel(Xf_test, None,    log_theta, 'Kff_test', dtype=precision)\n",
      "Kuu = kfactory.kernel(Xu, None,    log_theta, 'Kuu', dtype=precision)\n",
      "Kfu_train = kfactory.kernel(Xf_train, Xu, log_theta, 'Kfu_train', dtype=precision)\n",
      "Kfu_test = kfactory.kernel(Xf_test, Xu, log_theta, 'Kfu_test', dtype=precision)\n",
      "cKuu, iKuu, logDetKuu = cholInvLogDet(\n",
      "    Kuu, M, jitterProtect.jitter)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 141
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iKuu.type"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 142,
       "text": [
        "TensorType(float32, matrix)"
       ]
      }
     ],
     "prompt_number": 142
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def scan_ops(Kappa_sqrt):\n",
      "    output1 = T.tril(Kappa_sqrt - T.diag(T.diag(Kappa_sqrt)) + T.diag(T.exp(T.diag(Kappa_sqrt))))\n",
      "    output2 = T.diag(output1)\n",
      "    return [output1, output2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 143
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Variational distribution q(u)\n",
      "if analyticalPhiStats:\n",
      "    # if kernelType == 'RBF':\n",
      "    #     # Compute batch Psi statistics\n",
      "    #     # CHECK IF WE SIMPLY REPLACE N WITH B FOR BATCH CASE\n",
      "    #     # CHECK THETAS ARE THE RIGHT WAY AROUND\n",
      "    #     S_qX =\n",
      "    #     psi0, psi1, psi2 = psicomputations(T.exp(log_theta[0,0]), T.exp(log_theta[0,1]), Xu, mu_qX.T, X_var, return_psi2_n=False):\n",
      "    raise RuntimeError('Not implemented')\n",
      "else:\n",
      "    # kappa = sharedZeroMatrix(M, P, '_kappa', dtype=precision)\n",
      "    # Kappa_sqrt = sharedZero3Tensor(P, M, M, '_Kappa_sqrt', dtype=precision)\n",
      "    # ([Kappa_L,Kappa_diags], updates) = th.scan(scan_ops,\n",
      "    #                                     sequences=[Kappa_sqrt],\n",
      "    #                                     outputs_info=None)\n",
      "    # # Kappa_sqrt = sharedZeroMatrix(M, M, 'Kappa_sqrt', dtype=precision)\n",
      "    # # Kappa = dot(Kappa_sqrt, Kappa_sqrt.T, 'Kappa')\n",
      "    # gradientVariables.extend([kappa,Kappa_sqrt])\n",
      "    # # Kappa_conditionNumber = conditionNumber(Kappa)\n",
      "    # mu_qu = kappa\n",
      "\n",
      "    kappa = sharedZeroMatrix(M, P, '_kappa', dtype=precision)\n",
      "    Kappa_sqrt_shared = sharedZeroMatrix(M, M, '_Kappa_sqrt_shared', dtype=precision)\n",
      "    Kappa_sqrt = T.tril(Kappa_sqrt_shared - T.diag(T.diag(Kappa_sqrt_shared)) + T.diag(T.exp(T.diag(Kappa_sqrt_shared))))\n",
      "    Kappa = T.dot(Kappa_sqrt, Kappa_sqrt.T)\n",
      "    \n",
      "    gradientVariables.extend([kappa,Kappa_sqrt_shared])\n",
      "    \n",
      "\n",
      "    u = kappa + T.dot(Kappa_sqrt, gamma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Variational distribution\n",
      "# A has dims [BxM] = [BxM] . [MxM]\n",
      "A_train = dot(Kfu_train, iKuu, 'A_train')\n",
      "A_test = dot(Kfu_test, iKuu, 'A_test')\n",
      "# Sigma is the covariance of conditional distribution q(z|Xf)\n",
      "Sigma_train = minus(Kff_train,\n",
      "                   plus(dot(A_train, Kfu_train.T),\n",
      "                        dot(A_train, dot(Kappa, A_train.T))), 'Sigma_train')\n",
      "Sigma_test = minus(Kff_test,\n",
      "                   plus(dot(A_test, Kfu_test.T),\n",
      "                        dot(A_test, dot(Kappa, A_test.T))), 'Sigma_test')\n",
      "cSigma_train, iSigma_train, logDetSigma_train \\\n",
      "    = cholInvLogDet(Sigma_train, B, jitterProtect.jitter)\n",
      "cSigma_test, iSigma_test, logDetSigma_test \\\n",
      "    = cholInvLogDet(Sigma_test, B, jitterProtect.jitter)\n",
      "mu_train = dot(A_train, kappa, 'mu_train')\n",
      "mu_test = dot(A_test, kappa, 'mu_test')\n",
      "# Sample f from q(f|X) = N(mu, Sigma)\n",
      "f_train = plus(mu_train, (dot(cSigma_train, beta)), 'f_train')\n",
      "f_test = plus(mu_test, (dot(cSigma_test, beta)), 'f_test')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for drawing samples\n",
      "func_f = th.function([Xf], f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'th' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-146-8ac18744a7dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for drawing samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfunc_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'th' is not defined"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Kuu_conditionNumber   = conditionNumber(Kuu)\n",
      "# Sigma_conditionNumber = conditionNumber(Sigma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 147
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def construct_L_terms(Xf, mu_qXf, sigma_qXf, u, f, y_miniBatch):\n",
      "    \n",
      "    log_py = log_normal(y_miniBatch, f, sigma_y).sum(axis=1)\n",
      "\n",
      "    if analyticalPhiStats:\n",
      "        raise RuntimeError('Not implemented')\n",
      "    else:\n",
      "        KL_u = 0.5 * T.sum(T.square(kappa)) # Mahalanobis term\n",
      "        KL_u += -0.5 * T.cast(T.prod(T.shape(Kappa_sqrt)[0:2]), precision) # constant term\n",
      "        KL_u -= 0.5 * T.sum(T.log(T.square(T.diag(Kappa_sqrt)))) # logdet term\n",
      "        KL_u += 0.5 * T.sum(T.square(Kappa_sqrt)) # trace term\n",
      "    \n",
      "    if enc_type == 'MLP_diag':\n",
      "\n",
      "#             H_qXf = 0.5 * R * B * (1 + log2pi) \\\n",
      "#                 + R * T.sum(log_sigma_qXf)\n",
      "#             H_qXf.name = 'H_qXf'\n",
      "\n",
      "#             log_pXf = log_stdnormal(Xf).sum(axis=1)\n",
      "\n",
      "#             KL_Xf = -H_qXf - log_pXf\n",
      "        KL_Xf = kl_normal1_stdnormal(mu_qXf, sigma_qXf, eps=nan_noise).sum(axis=1)\n",
      "    else:\n",
      "        RuntimeError('Not implemented')\n",
      "            \n",
      "\n",
      "    LL = T.mean(log_py - KL_Xf) - KL_u\n",
      "    return LL, T.mean(log_py), KL_u, T.mean(KL_Xf)\n",
      "#     return log_py, KL_u, KL_Xf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def randomise(gradientVariables, rnd, sig=1.0):\n",
      "\n",
      "    def rndsub(var):\n",
      "        if type(var) == np.ndarray:\n",
      "            return np.asarray(sig * rnd.randn(*var.shape), dtype=precision)\n",
      "        elif type(var) == T.sharedvar.TensorSharedVariable:\n",
      "            if var.name.startswith('_'):\n",
      "                print 'Randomising ' + var.name + ' normal random variables'\n",
      "                var.set_value(rndsub(var.get_value()))\n",
      "            else:\n",
      "                pass # Parmesan parameters are already initialised\n",
      "        elif type(var) == T.sharedvar.ScalarSharedVariable:\n",
      "            print 'Randomising ' + var.name\n",
      "            var.set_value(rnd.randn*sig)\n",
      "        else:\n",
      "            raise RuntimeError('Unknown randomisation type')\n",
      "\n",
      "#     members = [attr for attr in dir(gradientVariables)]\n",
      "\n",
      "    for var in gradientVariables:\n",
      "        if type(var) == T.sharedvar.ScalarSharedVariable or \\\n",
      "           type(var) == T.sharedvar.TensorSharedVariable:\n",
      "            rndsub(var)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 149
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "randomise(gradientVariables, np.random)\n",
      "\n",
      "lower_bound_train, log_py_train, KL_u_train, KL_Xf_train = construct_L_terms(Xf_train, mu_qXf_train, sigma_qXf_train, u, f_train, sym_y)\n",
      "lower_bound_test, log_py_test, KL_u_test, KL_Xf_test = construct_L_terms(Xf_test, mu_qXf_test, sigma_qXf_test, u, f_test, sym_y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Randomising _log_theta normal random variables\n",
        "Randomising _log_sigma_y normal random variables\n",
        "Randomising _Xu normal random variables\n",
        "Randomising _kappa normal random variables\n",
        "Randomising _Kappa_sqrt_shared normal random variables\n"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"lower_bound_train\", jitterProtect.jitterProtect2(lower_bound_train, {sym_y:Y})\n",
      "print \"log_py\", jitterProtect.jitterProtect2(log_py_train, {sym_y:Y})\n",
      "\n",
      "line = ''\n",
      "\n",
      "for p in gradientVariables:\n",
      "    print p, p.get_value().shape, p.type\n",
      "    line += \"%s %s\\n\" % (p, str(p.get_value().shape))\n",
      "\n",
      "# with open(trainlogfile,'w') as f:\n",
      "#     f.write(\"Trainlog\\n\")\n",
      "\n",
      "# with open(logfile,'a') as f:\n",
      "#     f.write(line)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "lower_bound_train "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jitter was increased to 2.68545722961\n",
        "-40644.3515625\n",
        "log_py "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jitter was increased to 2.44132471085\n",
        "-19815.0371094\n",
        "_log_theta (1, 6) TensorType(float32, row)\n",
        "_log_sigma_y () TensorType(float32, scalar)\n",
        "Dense-ENC_A_DENSE0_1.W (784, 100) TensorType(float32, matrix)\n",
        "Dense-ENC_A_DENSE0_1.b (100,) TensorType(float32, vector)\n",
        "SaS-ENC_A_DENSE0_1.beta (1, 100) TensorType(float32, matrix)\n",
        "SaS-ENC_A_DENSE0_1.gamma (1, 100) TensorType(float32, matrix)\n",
        "Dense-ENC_A_MU0.W (100, 5) TensorType(float32, matrix)\n",
        "Dense-ENC_A_MU0.b (5,) TensorType(float32, vector)\n",
        "SaS-ENC_A_MU0.beta (1, 5) TensorType(float32, matrix)\n",
        "SaS-ENC_A_MU0.gamma (1, 5) TensorType(float32, matrix)\n",
        "Dense-ENC_A_var0.W (100, 5) TensorType(float32, matrix)\n",
        "Dense-ENC_A_var0.b (5,) TensorType(float32, vector)\n",
        "SaS-ENC_A_var0.beta (1, 5) TensorType(float32, matrix)\n",
        "SaS-ENC_A_var0.gamma (1, 5) TensorType(float32, matrix)\n",
        "_Xu (50, 5) TensorType(float32, matrix)\n",
        "_kappa (50, 784) TensorType(float32, matrix)\n",
        "_Kappa_sqrt_shared (50, 50) TensorType(float32, matrix)\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lbtmp = jitterProtect.jitterProtect2(lower_bound_train, {sym_y:Y}, reset=False)\n",
      "cost = -lower_bound_train # now with an acceptable jitter value\n",
      "jitterProtect.reset()\n",
      "\n",
      "grads = T.grad(cost, gradientVariables)\n",
      "clip_grad = 0.9 # changed here from $\n",
      "max_norm = 4 # changed here from 5\n",
      "mgrads = lasagne.updates.total_norm_constraint(grads,max_norm=max_norm)\n",
      "cgrads = [T.clip(g,-clip_grad, clip_grad) for g in mgrads]\n",
      "\n",
      "updates = lasagne.updates.adam(cgrads, gradientVariables,beta1=0.9, beta2=0.999, epsilon=1e-4, learning_rate=sym_lr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jitter was increased to 2.21938610077\n"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_model = theano.function([sym_y, sym_lr],\n",
      "                              [lower_bound_train] +\n",
      "                              [log_py_train] +\n",
      "                              [KL_u_train] +\n",
      "                              [KL_Xf_train] +\n",
      "                              [u] +\n",
      "                              [mu_train] +\n",
      "                              [Sigma_train] +\n",
      "                              [Xf_train] +\n",
      "                              [f_train],\n",
      "                              updates=updates)\n",
      "\n",
      "\n",
      "# test_model = theano.function([sym_y, sym_lr],\n",
      "#                               [lower_bound_test] +\n",
      "#                               [log_py_test] +\n",
      "#                               [KL_u_test] +\n",
      "#                               [KL_Xf_test] +\n",
      "#                               [u] +\n",
      "#                               [mu_test] +\n",
      "#                               [Sigma_test] +\n",
      "#                               [Xf_test] +\n",
      "#                               [f_test] +\n",
      "#                               [y_mu_test])\n",
      "\n",
      "# test_model5000 = theano.function([sym_y, sym_lr],\n",
      "#                               [lower_bound_test] +\n",
      "#                               [log_py_test] +\n",
      "#                               [KL_u_test] +\n",
      "#                               [KL_Xf_test])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "if batch_norm:\n",
      "    try:\n",
      "        collect_y = process_data(collect_y) #if defined use the sh_y_collect for bn\n",
      "    except:\n",
      "        collect_y = process_data(train_y)  #else just use the full training data\n",
      "    collect_out = lasagne.layers.get_output([l_Xf, lenc_Xf_mu, lenc_Xf_var], sym_y, deterministic=False, collect=True)\n",
      "    f_collect = theano.function([sym_y],\n",
      "                                collect_out)\n",
      "\n",
      "\n",
      "n_train_batches = train_y.shape[0] / batch_size\n",
      "#n_valid_batches = valid_y.shape[0] / batch_size_val\n",
      "n_test_batches = test_y.shape[0] / batch_size_test"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Collecting using single pass...\n",
        "Collecting using single pass...\n",
        "Collecting using single pass...\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_epoch(y, lr, epoch):\n",
      "    costs, log_py = [],[],\n",
      "    KL_u = None\n",
      "    KL_Xf = None\n",
      "    mu_qXf = []\n",
      "    log_sigma_qXf = []\n",
      "    mu_f = []\n",
      "    var_f = []\n",
      "    Xf_sample = []\n",
      "    f_sample = []\n",
      "\n",
      "\n",
      "    for i in range(n_train_batches):\n",
      "        y_batch = y[i*batch_size:(i+1)*batch_size]\n",
      "        #if epoch == 1:\n",
      "        #    lr = lr*1.0/float(n_train_batches-i)\n",
      "        out = jitterProtect.jitterProtect(train_model, [y_batch,lr])\n",
      "        \n",
      "\n",
      "        costs += [out[0]]\n",
      "        log_py += [out[1]]\n",
      "#         KL_u += [out[2]]\n",
      "#         KL_Xf += [out[3]]\n",
      "        verbose_print([str(i)] + map(lambda s: \"%0.2f\"%s,[out[0]]+ [out[1]] + out[2] + [out[3]]))\n",
      "        if KL_u == None:\n",
      "            KL_u = [out[2]]\n",
      "        else:\n",
      "#             KL_u = [old+new for old,new in zip(KL_u, out[2])]\n",
      "            KL_u += [out[2]]\n",
      "\n",
      "        if KL_Xf == None:\n",
      "            KL_Xf = [out[3]]\n",
      "        else:\n",
      "#             KL_Xf = [old+new for old,new in zip(KL_Xf, out[3])]\n",
      "            KL_Xf += [out[3]]\n",
      "\n",
      "        # if epoch in eval_epochs:\n",
      "        #     mu_p_batch = out[2+3*num_layers:1+4*num_layers]\n",
      "        #     var_p_batch = out[1+4*num_layers:0+5*num_layers]\n",
      "        #     for j,mu in enumerate(mu_p_batch):\n",
      "        #         mu_p[j][i*batch_size:(i+1)*batch_size] = mu.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "        #     for j,var in enumerate(var_p_batch):\n",
      "        #         var_p[j][i*batch_size:(i+1)*batch_size] = var.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "        #     mu_q_batch = out[0+5*num_layers:0+6*num_layers]\n",
      "        #     var_q_batch = out[0+6*num_layers:0+7*num_layers]\n",
      "        #     for j,mu in enumerate(mu_q_batch):\n",
      "        #         if reversed_z:\n",
      "        #             mu_q[j][i*batch_size:(i+1)*batch_size] = mu.reshape((-1,1,1,latent_sizes[j])) if j == num_layers-1 else mu.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "        #         else:\n",
      "        #             mu_q[j][i*batch_size:(i+1)*batch_size] = mu.reshape((-1,1,1,latent_sizes[j])) if j == 0 else mu.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "\n",
      "        #     for j,var in enumerate(var_q_batch):\n",
      "        #         if reversed_z:\n",
      "        #             var_q[j][i*batch_size:(i+1)*batch_size] = var.reshape((-1,1,1,latent_sizes[j])) if j == num_layers-1 else var.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "        #         else:\n",
      "        #             var_q[j][i*batch_size:(i+1)*batch_size] = var.reshape((-1,1,1,latent_sizes[j])) if j == 0 else var.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "        #     z_batch = out[0+7*num_layers:0+8*num_layers]\n",
      "        #     for j,z in enumerate(z_batch):\n",
      "        #         z_sample[j][i*batch_size:(i+1)*batch_size] = z.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "    return np.mean(costs), np.mean(log_py,axis=0), \\\n",
      "           [KL/float(n_train_batches) for KL in KL_u], \\\n",
      "           [KL/float(n_train_batches) for KL in KL_Xf]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 102
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_epoch(y, eq_samples):\n",
      "    if batch_norm:\n",
      "        _ = f_collect(collect_x,1,1) #collect BN stats on train\n",
      "    costs, log_py = [],[],\n",
      "    KL_u = None\n",
      "    KL_Xf = None\n",
      "    mu_qXf = []\n",
      "    log_sigma_qXf = []\n",
      "    mu_f = []\n",
      "    var_f = []\n",
      "    Xf_sample = []\n",
      "    f_sample = []\n",
      "\n",
      "    model = test_model\n",
      "\n",
      "    for i in range(n_test_batches):\n",
      "        y_batch = y[i*batch_size_test:(i+1)*batch_size_test]\n",
      "        out = model(y_batch,lr)\n",
      "\n",
      "        costs += [out[0]]\n",
      "        log_py += [out[1]]\n",
      "#         KL_u += [out[2]]\n",
      "#         KL_Xf += [out[3]]\n",
      "        verbose_print([str(i)] + map(lambda s: \"%0.2f\"%s,[out[0]]+ [out[1]] + out[2] + [out[3]]))\n",
      "        if KL_u == None:\n",
      "            KL_u = [out[2]]\n",
      "        else:\n",
      "#             KL_u = [old+new for old,new in zip(KL_u, out[2])]\n",
      "            KL_u += [old[2]]\n",
      "\n",
      "        if KL_Xf == None:\n",
      "            KL_Xf = [out[3]]\n",
      "        else:\n",
      "#             KL_Xf = [old+new for old,new in zip(KL_Xf, out[3])]\n",
      "            KL_Xf += [out[3]]\n",
      "\n",
      "        # if iw_samples == 1 and eq_samples == 1: #dont want to do this for eq5000 since it is a lot of samples\n",
      "        #     mu_p_batch = out[2+3*num_layers:1+4*num_layers]\n",
      "        #     var_p_batch = out[1+4*num_layers:0+5*num_layers]\n",
      "        #     for j,mu in enumerate(mu_p_batch):\n",
      "        #         mu_p[j][i*batch_size_test:(i+1)*batch_size_test] = mu.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "        #     for j,var in enumerate(var_p_batch):\n",
      "        #         var_p[j][i*batch_size_test:(i+1)*batch_size_test] = var.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "        #     mu_q_batch = out[0+5*num_layers:0+6*num_layers]\n",
      "        #     var_q_batch = out[0+6*num_layers:0+7*num_layers]\n",
      "        #     for j,mu in enumerate(mu_q_batch):\n",
      "        #         mu_q[j][i*batch_size_test:(i+1)*batch_size_test] = mu.reshape((-1,1,1,latent_sizes[j])) if j == 0 else mu.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "        #     for j,var in enumerate(var_q_batch):\n",
      "        #         var_q[j][i*batch_size_test:(i+1)*batch_size_test] = var.reshape((-1,1,1,latent_sizes[j])) if j == 0 else var.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "        #     z_batch = out[0+7*num_layers:0+8*num_layers]\n",
      "        #     for j,z in enumerate(z_batch):\n",
      "        #         z_sample[j][i*batch_size_test:(i+1)*batch_size_test] = z.reshape((-1,eq_samples,iw_samples,latent_sizes[j]))\n",
      "\n",
      "\n",
      "\n",
      "    return np.mean(costs), np.mean(log_py,axis=0), \\\n",
      "           [KL/float(n_test_batches) for KL in KL_u], \\\n",
      "           [KL/float(n_test_batches) for KL in KL_Xf]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 103
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def init_res():\n",
      "   res = {}\n",
      "   res['cost'] = []\n",
      "   res['log_py'] = []\n",
      "   res['KL_u'] = []\n",
      "   res['KL_Xf'] = []\n",
      "   res['epoch'] = []\n",
      "   res['acc'] = []\n",
      "   return res\n",
      "\n",
      "def add_res(model_out,epoch,res):\n",
      "    cost, log_py, KL_u, KL_Xf = model_out\n",
      "    res['cost'] += [cost]\n",
      "    res['log_py'] += [log_py]\n",
      "    res['epoch'] += [epoch]\n",
      "    res['KL_u'] += [KL_u]\n",
      "    res['KL_Xf'] += [KL_Xf]\n",
      "    return res\n",
      "\n",
      "total_time_start = time.time()\n",
      "train_res = init_res()\n",
      "test1_res = init_res()\n",
      "test5000_res = init_res()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Training\"\n",
      "\n",
      "for epoch in range(1,num_epochs+1):\n",
      "    start = time.time()\n",
      "    #if epoch > 2000:\n",
      "    #    lr = lr*0.9995\n",
      "\n",
      "    np.random.shuffle(train_y)\n",
      "\n",
      "    train_out = train_epoch(process_data(train_y),lr, epoch)\n",
      "    costs_train_tmp, log_py_train_tmp, KL_u_tmp, KL_Xf_temp = train_out\n",
      "    t = time.time() - start\n",
      "    line = \"*Epoch=%i\\tTime=%0.2f\\tLR=%0.5f\\t\" %(epoch, t, lr) + \\\n",
      "        \"TRAIN:\\tCost=%0.5f\\tlogp(y|f)=%0.5f\\t\"%(costs_train_tmp, log_py_train_tmp) + \\\n",
      "        \"KL_u: \" + \"|\".join(map(lambda s: \"%0.3f\"%s,KL_u_tmp)) + \"\\t\"  + \\\n",
      "        \"KL_Xf: \" + \"|\".join(map(lambda s: \"%0.3f\"%s,KL_Xf_temp))\n",
      "\n",
      "    print line\n",
      "    with open(trainlogfile,'a') as f:\n",
      "        f.write(line + \"\\n\")\n",
      "\n",
      "    if np.isnan(train_out[0]):\n",
      "        break\n",
      "\n",
      "    if epoch in eval_epochs:\n",
      "        t = time.time() - start #stop time so we only measure train time\n",
      "        print \"calculating L1, L5000\"\n",
      "\n",
      "        costs_train_tmp, log_py_train_tmp, KL_u_tmp, KL_Xf_temp = train_out\n",
      "        train_res = add_res(train_out,epoch,train_res)\n",
      "\n",
      "        test1_out = test_epoch(test_y)\n",
      "        costs_test1_tmp, log_py_test1_tmp, KL_u_test1, KL_Xf_test1 = test1_out\n",
      "        test1_res = add_res(test1_out,epoch,test1_res)\n",
      "\n",
      "        # test5000_out = test_epoch(test_y, 5000)\n",
      "        # costs_test5000_tmp, log_py_test5000_tmp, KL_u_test5000, KL_Xf_test5000 = test5000_out\n",
      "        # test5000_res = add_res(test5000_out,epoch,test5000_res)\n",
      "\n",
      "        with open(res_out + '/res.cpkl','w') as f:\n",
      "            # cPickle.dump([train_res,test1_res,test5000_res],f,protocol=cPickle.HIGHEST_PROTOCOL)\n",
      "            cPickle.dump([train_res,test1_res],f,protocol=cPickle.HIGHEST_PROTOCOL)\n",
      "\n",
      "\n",
      "        if drawsamples:\n",
      "            print \"drawing samples\"\n",
      "            Xf_samples = np.random.randn(B,R)\n",
      "            f_samples = func_f(Xf_samples)\n",
      "            plotsamples('samples_prior%i'%epoch,res_out,f_samples.reshape([-1]+imgshp))\n",
      "            if enc_type == 'MLP':\n",
      "                Xf_samples_sym = lasagne.layers.get_output(l_Xf,{l_in:sym_y}, deterministic=True, drawdecsample=True)\n",
      "                func_cond_sample = th.function([sym_y], f, givens={(Xf, Xf_samples_sym)})\n",
      "                # th.clone(f, replace = {Xf: Xf_samples_sym})\n",
      "                f_samples = func_cond_sample(test_y[:B])\n",
      "                plotsamples('samples_conditioned%i'%epoch,res_out,f_samples.reshape([-1]+imgshp))\n",
      "            elif enc_type == 'None':\n",
      "                f_samples = f.eval()\n",
      "                plotsamples('samples_post_uncond%i'%epoch,res_out,f_samples.reshape([-1]+imgshp))\n",
      "\n",
      "        #dump model params\n",
      "        # all_params=lasagne.layers.get_all_param_values(outputlayers)\n",
      "        all_params = gradientVariables\n",
      "        f = gzip.open(model_out + 'epoch%i'%(epoch), 'wb')\n",
      "        cPickle.dump(all_params, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
      "        f.close()\n",
      "\n",
      "        # LOGGING, SAVING MODEL and PLOTTING\n",
      "\n",
      "        line = \"*Epoch=%i\\tTime=%0.2f\\tLR=%0.5f\\t\" %(epoch, t, lr) + \\\n",
      "            \"TRAIN:\\tCost=%0.5f\\tlogp(y|f)=%0.5f\\t\"%(train_res['cost'][-1], train_res['log_py'][-1]) + \\\n",
      "            \"KL_u: \" + \"|\".join(map(lambda s: \"%0.3f\"%s,KL_u_tmp)) + \"\\t\"  + \\\n",
      "            \"KL_Xf: \" + \"|\".join(map(lambda s: \"%0.3f\"%s,KL_Xf_tmp)) + \"\\t\"  + \\\n",
      "            \"TEST-1:\\tCost=%0.5f\\tlogp(y|f)=%0.5f\\t\"%(test1_res['cost'][-1], test1_res['log_py'][-1]) + \\\n",
      "            \"KL_u: \" + \"|\".join(map(lambda s: \"%0.3f\"%s,KL_u_test1)) + \"\\t\"  + \\\n",
      "            \"KL_Xf: \" + \"|\".join(map(lambda s: \"%0.3f\"%s,KL_Xf_test1)) + \"\\t\"  #+ \\\n",
      "            # \"TEST-5000:\\tCost=%0.5f\\tlogp(x|z1)=%0.5f\\t\"%(test5000_res['cost'][-1], test5000_res['log_px'][-1]) + \\\n",
      "            # \"log p(z): \" + \"|\".join(map(lambda s: \"%0.3f\"%s,log_pz_cur_test5000)) + \"\\t\"  + \\\n",
      "            # \"log q(z): \" + \"|\".join(map(lambda s: \"%0.3f\"%s,log_qz_cur_test5000)) + \"\\t\" + \\\n",
      "            # \"%0.5f\\t%0.5f\\t%0.5f\" %(train_res['cost'][-1],test1_res['cost'][-1],test5000_res['cost'][-1])\n",
      "\n",
      "\n",
      "        print line\n",
      "\n",
      "        with open(logfile,'a') as f:\n",
      "            f.write(line + \"\\n\")\n",
      "\n",
      "        # plotLLs('Train_LLs',res_out,train_res['epoch'],train_res['cost'],train_res['log_px'],train_res['log_pz'],train_res['log_qz'])\n",
      "        # plotLLs('Test1_LLs',res_out,test1_res['epoch'],test1_res['cost'],test1_res['log_px'],test1_res['log_pz'],test1_res['log_qz'])\n",
      "        # plotLLs('Test5000_LLs',res_out,test5000_res['epoch'],test5000_res['cost'],test5000_res['log_px'],test5000_res['log_pz'],test5000_res['log_qz'])\n",
      "        # for i,KL in enumerate(train_res['KL_qp']):\n",
      "        #     plotKLs('Train_KL_z%i'%i,res_out,train_res['epoch'],KL)\n",
      "\n",
      "        # for i,KL in enumerate(test1_res['KL_qp']):\n",
      "        #     plotKLs('Test1_KL_z%i'%i,res_out,test1_res['epoch'],KL)\n",
      "\n",
      "        # for i,KL in enumerate(test5000_res['KL_qp']):\n",
      "        #     plotKLs('Test5000_KL_z%i'%i,res_out,test5000_res['epoch'],KL)\n",
      "\n",
      "        # plt.close(\"all\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training\n",
        "Jitter was increased to 6.96538543701"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['0', 'nan', 'nan']\n",
        "Jitter was increased to 7.66192436218"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['1', 'nan', 'nan']\n",
        "Jitter was increased to 6.96538543701"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['2', 'nan', 'nan']\n",
        "Jitter was increased to 6.96538543701"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['3', 'nan', 'nan']\n",
        "Jitter was increased to 6.96538543701"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['4', 'nan', 'nan']\n",
        "Jitter was increased to 6.96538543701"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['5', 'nan', 'nan']\n",
        "Jitter was increased to 7.66192436218"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['6', 'nan', 'nan']\n",
        "Jitter was increased to 6.3321685791"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['7', 'nan', 'nan']\n",
        "Jitter was increased to 7.66192436218"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['8', 'nan', 'nan']\n",
        "Jitter was increased to 7.66192436218"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['9', 'nan', 'nan']\n",
        "Jitter was increased to 6.96538543701"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "['10', 'nan', 'nan']\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-105-d621dbb754bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcosts_train_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_py_train_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKL_u_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKL_Xf_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-102-728200fe6815>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(y, lr, epoch)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#if epoch == 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m#    lr = lr*1.0/float(n_train_batches-i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjitterProtect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjitterProtect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/roryjbeard/DPhil/GP-LVM-Conditional MF/jitterProtect.py\u001b[0m in \u001b[0;36mjitterProtect\u001b[0;34m(self, func, func_args, reset)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpassed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.random.randn(2,3,3)\n",
      "B = np.random.randn(2,3,3)\n",
      "Bt = np.transpose(B, (0, 2, 1))\n",
      "C = np.dot(A,Bt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print Bt.shape\n",
      "print C.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(2, 3, 3)\n",
        "(2, 3, 2, 3)\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "AA = np.random.randn(3,3,2)\n",
      "BB = np.random.randn(3,3,2)\n",
      "CC = np.dot(A,B)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print CC.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(2, 3, 2, 3)\n"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "AA = T.dmatrix()\n",
      "BB = T.dmatrix()\n",
      "cc = T.dot(AA,BB)\n",
      "funcc = theano.function([AA, BB], cc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "_AA = np.random.randn(3,3)\n",
      "_BB = np.random.randn(3,3)\n",
      "funcc_args = [_AA, _BB]\n",
      "funcc(_AA, _BB)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "array([[-1.30230114, -0.06605723, -1.05523011],\n",
        "       [ 0.4595159 ,  0.25231072,  0.90008717],\n",
        "       [-0.63570817,  0.1681496 , -0.17819325]])"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "funcc(*funcc_args)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 39,
       "text": [
        "array([[-1.30230114, -0.06605723, -1.05523011],\n",
        "       [ 0.4595159 ,  0.25231072,  0.90008717],\n",
        "       [-0.63570817,  0.1681496 , -0.17819325]])"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
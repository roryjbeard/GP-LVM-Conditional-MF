\documentclass[]{article}
\usepackage{proceed2e}
% Set the typeface to Times Roman
\usepackage{times}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath}
\usepackage{amssymb}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\chris}[1]{\textcolor{ForestGreen}{#1}}
\newcommand{\Tau}{\mathcal{T}}
\newcommand{\Kappa}{\mathcal{K}}
\newcommand{\xb}{\mathbf{x}}
\newcommand{\ub}{\mathbf{u}}
\newcommand{\zb}{\mathbf{z}}
\newcommand{\fb}{\mathbf{f}}
\newcommand{\Cb}{\mathbf{C}}
\newcommand{\Ab}{\mathbf{A}}
\newcommand{\yb}{\mathbf{y}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\Lo}{\mathcal{L}_1}
\newcommand{\Laux}{\mathcal{L}_{\mathrm{aux}}}
\newcommand{\Kff}{\mathbf{K}_{ff}}
\newcommand{\Kuu}{\mathbf{K}_{uu}}
\newcommand{\Kuf}{\mathbf{K}_{uf}}
\newcommand{\Kfu}{\mathbf{K}_{fu}}
\newcommand{\Ex}{\mathbb{E}}
\newcommand{\KL}{\mathrm{KL}}
\newcommand{\No}{\mathcal{N}}
\newcommand{\chol}{\mathrm{chol}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\Wb}{\mathbf{W}}
\newcommand{\hb}{\mathbf{h}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bx}{\mathbf{x}}

% Just a working title/suggestion
\title{Black-box Auxiliary Variational Inference}

%\author{} % LEAVE BLANK FOR ORIGINAL SUBMISSION.
          % UAI  reviewing is double-blind.

\author{ {\bf Rory Beard} \\
\And
{\bf Chris Lloyd}  \\
\And
{\bf Stephen J. Roberts}   \\
\AND
\textnormal{Machine Learning Research Group} \\
\textnormal{Department of Engineering Science}\\
\textnormal{University of Oxford} \\
\url{{rory, clloyd, sjrob}@robots.ox.ac.uk}
}

\begin{document}

\maketitle

\begin{abstract}
We approach the problem of capturing the statistical dependence structure of the conditionally independent variational factors using a flexible, auxiliary variable method. We introduce two flexible auxiliary inference models; a deep neural net and a Bayesian non-paramteric model. We compare the performance of these models against each other and also consider a hybrid approach. The power of the approach lies in its ability to combine bottom-up and top-down learning in a fully Bayesian manner via a bidirectional augmented inference network.
\end{abstract}

\section{Introduction}\label{sec:intro}


Considering at first a latent variable model with a latent variable for each datum as well as some global parameters, a traditional (mean field) variational approach is to learn a factored approximation to the posterior in which each latent variable is independently distributed and governed by its own variational parameter.
Clearly this is an overly restrictive approximation since conditioning on the data statistically couples the latent variables.
With careful design some statistical dependence may be retained with so-called \emph{structured mean field} approximations (SAUL AND JORDAN, WIERGERNINCK), but the design is highly model specific and not AUTOMATICALLY SCALABLE?
A natural improvement is to couple the variational factors via a common variational prior, leading to a hierarchical variational approximation (RANGANATH 2015) which are richer and more expressive.
HOWEVER THE NECESSARY CRUCIAL DESIGN CHOICES MAKE THIS PROCEDURE DIFFICULT TO AUTOMATE AND SCALE?

Rather than designing model specific, parametric variational priors..., we take inspiration from the Bayesian non-parametric dimensionality reduction method of.. GP-LVM... to learn a flexible, non-linear mapping from a shared latent auxiliary space to each conditionally independent variational factor.
The dependence is induced by a fixed number of latent coordinates, the locations of which are learned so as to induce the optimal dependence structure. We refer to this scheme as GP-LVM-conditional mean field (GP-LVM-CMF) inference.

From the perspective of the variational principle, we propose a drop-in replacement for mean field inference in which a rich, variational distribution is specified in the expanded space of variables we wish to infer, augmented with auxiliary inference variables.
This can be seen as specifying a non-parametric hierarchical variational model (CITE RANAGNATH 2015)... integrating over the auxiliary inference parameters recovers a marginal distribution as an infinite mixture of augmented distributions...

From the perspective of recognition models and variational autoencoding, the GP-LVM-CMF scheme consists of a non-parametric inference network, or \emph{encoder}, which maps the augmented data-auxiliary variable space to the latent variable space. ... should be more flexible than rigidly encoding directly from the non-augmented data space (WHY EXACTLY???)

In addition to this encoder model, we introduce a framework for training variational auto-encoders and their more complex derived forms. Of particular interest are deep VAE architectures that consist of stacked stochastic hidden layers in both the decoding and encoding arms. While the depth of the generative model allows for richer latent representations and generative capacity, learning the model parameters via an equivalent deep encoder leads to returns which sharply diminish as more stochastic layers are added. We demonstrate that it is the unidirectional nature of the flow of information during training that hampers the learning process, causing the hidden layers closest to the data to fit rigidly to the data, rendering model hidden units further upstream effectively inactive. To remedy this pathology we propose an auxiliary inference scheme which combines the bottom-up stochastic message passing of the vanilla encoder with top-down flowing information. We show that this Auxiliary Variational Inference (AVI) scheme significantly outperform the now standard stochastic backpropagation procedure for architectures of varying depth.
The focus of this work is to inform on how to correctly learn deep, expressive generative models, rather than on the design of such generative models themselves. Therefore the experiments documented here were designed to showcase the ability of the proposed inference scheme to boost the performance of a \emph{given} model.

Pleasingly the technique of furnishing the variational encoder with an auxiliary model is not a heuristic or hack like e.g. the popular tricks of dropout, layer-wise pre-training, gradient clipping, and more recently batch normalisation, rather it is manifest as the result of natural application of the Bayesian variational principle. In particular the technique involves further lower bounding the variational lower bound of the model evidence in a way which recovers the auxiliary lower bound considered in RERERENCE SALIMANS.


The marginal variational distribution over the variables of interest, $q(Z)$, is itself intractable precluding analyticity of the variational entropy in the lower bound therefore we lower bound again...

SVI framework permits a learning procedure that requires only unbiased estimates of the lower bound (or rather its gradient), removing the issue of the lack of analyticity of the lower bound and, for no extra cost, granting a highly data-scalable mini-batch update scheme.
There has been much recent work which empirically demonstrates the success of ... despite the presence of two sources of stochasticity.

Controlling the variance of the stochastic lower bounds: reparameterization trick... theoretical Lipschitz result, ease of implementation.


We additionally introduce a kernel based approach to back-constrained amortization in which random functions are trained to capture the encoding process.
WHY IS THIS GOOD: provable guarantees of kernel mappings, ameliorates pathological curvature??
This also provokes a second look at the VAE in which, in addition to replacing the decoder with a Bayesian non-parametric mapping, the same adjustment is made for the decoding part of the architecture.

We showcase the GP-LVM-CMF scheme ...
Experiments:

\section{Background principles}\label{sec:background}

\subsection{Variational Inference}\label{subsec:vi}

\begin{itemize}
  \item Fixed form and factorised... fixed-point coordinate ascent update scheme. Loss of posterior statistical dependence - particularly bad for factor models... --> motivating the IBP experiments.
  \item Black-box approaches and SVI. Reparam trick for variance reduction, autoencoding and back-constraints for amortization.
  \item Augmented inference space, hierarchical variational distribution.
\end{itemize}

\subsubsection{Variational Auto-encoding}

\begin{itemize}
  \item Provide a Bayesian regularisation
\end{itemize}
The VAE (2 REFERENCES) was introduced as a framework to train (potentially) deep neural net generative models in a scalable, Bayesian manner. The important insight of these authors was to reparameterise the variational variables as a deterministic transformation of a random variable drawn from a standard (parameterless) distribution so that gradient information relating to the variational parameters can be included in the stochastic gradients used to optimise the bound. Importantly this allows for variance control of the stochastic gradients, permitting practical convergence profiles, unlike the previously proposed sleep wake algorithm (REFERENCES).
\\ |
\\ |
\\ |
IMPORTANT VAE EQUATIONS
\\ |
\\ |
\\ |

Burda et al. REFERENCE extended the VAE by including one or more stochastic hidden layers between the data and the top layer latent variables, resulting in the following hierarchical specification:
%
\begin{align}
  p_\theta(\zb_l|\zb_{l+1}) &= \No(\zb_l\mid \mu_l(\zb_{l+1}), \sigma^2_l(\zb_{l+1}))\quad l\in[1,L] \\
  p_\theta(\zb_L) &= \No(\zb_L|0, I)
\end{align}

where $\mu_l$ and $\sigma^2_l$ are deterministic mappings consisting of a (often 2 hidden layer) MLP. Chaining together these simple dense mappings permits a highly correlated latent dependence structure without forgoing the computational efficiency and scalability of fully factorised models.

The inference network (encoder) used in the IWAE has simply the same architecture as the generative network but with mappings in the reverse direction (figure FIGURE) i.e.
%
\begin{align}
  q_\phi(\zb_1|\yb) &= \No(\zb_1\mid \mu(\yb), \sigma^2(\yb)) \\
  q_\phi(\zb_l|\zb_{l-1}) &= \No(\zb_l\mid \mu(\zb_{l-1}), \sigma^2(\zb_{l-1}))\quad l\in[2,L].
\end{align}
%
Applying the reparameterisation trick at each stochastic layer now amounts to first sampling from a standard normal $\boldsymbol{\xi}_l\sim\No(0,I)$ and then applying a deterministic mapping as follows:
%
\begin{equation}
  \zb_l(\xi_l,\zb_{l-1},\phi) = \mu(\zb_{l-1},\phi) + \sigma(\zb_{l-1},\phi)\odot\boldsymbol{\xi}_l
\end{equation}
%
Burda et al. optimise an evidence lower bound which differs from the traditional free energy in that it cannot be written as the difference between the evidence and a K-L divergence. This alternative bound highlights the fundamental similarity between variational inference and importance sampling, and in doing so begins to address the issues of the heavy penalisation under the standard bound of samples which poorly explain the data. However this penalisation worsens for layers which are further from the data as they do not receive a learning signal strong enough to overcome the K-L divergence penalisation that encourages variational posteriors towards their model priors.
%
\subsection{Auxiliary Variational Inference}\label{subsec:aux}

To overcome the pathologies of the standard inference scheme, we introduce auxiliary variables $\xb$ into the inference model as shown in figure FIGURE, which mediate a direct forward pass sampling route from the data to the top level latent variables. Since these variables do not appear in the generative model their effect must be marginalised out of the inference model. This marginalisation is however intractable and so we must further lower the bound of equation EQUATION by substituting in the Bayesian identity $q_\phi(\zb_1,\zb_2|\yb) = \frac{q_\phi(\zb_1|\yb)q_\phi(\xb|\yb)q_\phi(\zb_2|\zb_1,\xb)}{q_\phi(\xb|\zb_1,\zb_2,\yb)}$
%
\subsection{The Bayesian GP-LVM}\label{subsec:gplvm}

The GP-LVM was proposed by Lawrence as a Bayesian non-linear dimensionality reduction model and can be viewed as a multi-output GP regression model in which the inputs are unobserved and so treated as latent variables.
In the original framework these inputs were optimised rather than integrated out for tractability.

Denoting the $d^{th}$ column of the data matrix, $Y$, as $\mathbf{y}_d$ the likelihood function for the GP-LVM is
%
\begin{align}
  p(Y|X) &= \prod_{d=1}^D p(\mathbf{y}_d | X),\\
  p(\mathbf{y}_d|X) &= \mathcal{N}(\mathbf{y}_d|\mathbf{0}, K_{ff}+\beta^{-1}I)
\end{align}
%
where the GPs are modelled as independent across the data dimensions.

\begin{itemize}
  \item GP-LVM
  \item variational compression and SVGP.
  \item Bayesian GP-LVM
  \item Provides an extremely general, Bayesian and rich way of auxiliary object upon which to condition the mean field factors in order to couple them statistically.
\end{itemize}

\section{GP-LVM-Conditional Mean Field Variational Inference} \label{sec:gplvmcmf}

As is the case for the SVGP and the Bayesian GP-LVM, the inference model of the GP-LVM-CMF constructs a GP mapping to $N$ output values which is governed by a fixed number of auxiliary inducing variables.
However the motivations for this conditional inducing structure are distinct in these three cases.
For the sparse GP the desire is scalability; in particular to circumvent the costly $\mathcal{O}(N^3)$ complexity of the covariance matrix inversion, which is then limited to $\mathcal{O}(N^2M)$ through variational compression.
For the Bayesian GP-LVM the motivation is to be able to tractably integrate over the uncertainty associated with the unknown latent input locations. For the GP-LVM-CMF however there are no observations of the GP function values since they do not exist in the generative model, so their purpose here is to act as \emph{pseudo-data} upon which the rest of the function values can be conditioned via the standard GP formulation.

-------------THIS SHOULD POSSIBLY GO IN THE INTRO--------------

\begin{itemize}
  \item Swapping out the directly parameterised $q(z)$ of a fixed-form VB distribution for the augmented GP-LVM-CMF distribution leads to an intractable lower bound.
  \item While it is true that we have already forsaken analyticity by performing mini-batch stochastic estimates of the lower bound - so it doesn't worsen things in the sense of analyticity that we can only sample the entropy, we still have variance control to consider.
  \item By judiciously further lower bounding we show that we can not only avoid such risks of large stoch gradient variance, but do so in a way which is more Bayesian than the typical case.
  \item This procedure results in an \emph{auxiliary} lower bound which was inspired by independent approaches of Salimans and Welling and an UNPUBLISHED TECHNICAL NOTE on reinterpreting the sparse variational Gaussian process framework of Titsias.
  \item In the present work we shed new light on this auxiliary approach, and argue that it as a more correct (in the Bayesian sense) approach to inference than simply sampling the entropy term in the bound, as well as than MCMC.
\end{itemize}

----------------------------------------------------------------

We begin by applying the standard variational principle to lower bound the marginal log likelihood as follows
%
\begin{align}
\log p_\theta(Y) &= \Ex_{q_\phi(Z)}[\log p_\theta(Y|Z) - \log q_\phi(Z)]\notag\\
&+ \text{KL}[q_\phi(Z)\Vert p_\theta(Z|Y)]\label{eq:var_principle}\\
&\geq \Ex_{q_\phi(Z)}[\log p_\theta(Y|Z) - \log q_\phi(Z)] \notag \\
&\triangleq \mathcal{L}_1 \label{eq:bound1}
\end{align}
%
where we denote the set of all fixed point generative (resp. inference) model parameters to be learned by $\theta$ (resp. $\phi$), and we denote the set of all latent variables and parameters subject to inference by $Z$ (though later we will use $\eta$ to ...).
Integrating \ref{eq:bound1} with respect to the auxiliary variables, noting that the marginal variational distribution over $Z$ can be expressed as
\begin{equation}
q(Z) = \frac{q(Z|\fb,U,X)q(\fb,U,X)}{q(\fb,U,X|Z)}
\end{equation}
%
and introducing an auxiliary model $r_\psi(\fb,U,X)$, we arrive at the following \emph{auxiliary lower bound}:
%
\begin{align}
\mathcal{L}_1 &\geq \Ex_{q(Z,\fb,U,X)}\big[ \log p(Y|Z)p(Z)r(\fb,U,X|Z) \notag \\
&- \log q(Z|\fb,X)q(\fb,U,X) \big] \notag \\
&+ \Ex_{q(Z)}\big[\KL[q(\fb,U,X|Z)\Vert r(\fb,U,X|Z)]\big] \label{eq:lowerlower} \\
&\triangleq \mathcal{L}_{aux}. \label{eq:L_aux}
\end{align}
%
To explain this bound, we find it useful to refer to $q(\fb,U,X)$ (resp. $q(\fb,U,X|Z)$) as the \emph{variational prior} (resp.) \emph{variational posterior} distributions over the auxiliary inference variables, the latter of which constitutes a posterior belief over the auxiliary variables having observed a (set of) sample(s), $Z$, generated under the measure $q$ endowed by the inference network.

This lower bound is indistinguishable from the one we would arrive at if we were to apply the variational principle to the problem of trying to estimate (lower bound) the log evidence of the augmented model $p(Z)p(Y|Z)r(U|Z)$: getting r exactly right (i.e. choosing $r(U|Z)=q(U|Z)$) would recover $\mathcal{L}_1$ but intractability precludes knowing $q(U|Z)$ so we must replace it with a tractable surrogate, and perform inference in the augmented space rather than in the marginal $Z$ space.
--> Find the form of the surrogate $r$ which leads to a variational inference task in an augmented space which is as equivalent as possible to performing variational inference in the collapsed space.

+++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++

\subsection{Scalable and Amortised Inference}\label{subsec:scale}

Hensman SVI GVGP does do some amortising since the global inducing variables provide a statistical conduit...

Having forsaken analyticity in favour a rich and flexibly variational distribution, we must resort to gradient based methods to optimising the auxiliary lower bound with respect to the set of all parameters; generative, variational and auxiliary $\omega=\{\theta,\phi,\psi\}$, with a simulation-based stochastic approach in which we seek to optimise an MC estimate of the auxiliary lower bound:
%
\begin{align}
  \sum_{j=1}^J [\log p(Y|Z^{(j)})p(Z^{(j)})...]\label{eq:stoch_L_aux}
\end{align}
%
SVI, while introduced as a means of performing variational inference on a stochastic lower bound when the source of stochasticity is the mini-batch approach, popularised the fact that one only requires an unbiased estimate of the lower bound (in addition to satisfying some basic conditions) to converge via stochastic gradient descent on the (local) minima of the true lower bound.
There has been much recent success in the way of performing so called \emph{doubly stochastic} variational inference........
Inspired by the success of the approach in the VAE / stochastic backprop in deep networks of learning generative and inference model parameters concurrently, we perform stochastic gradient-based optimization in the joint parameter space of $\omega$.
The gradients produced by naively differentiating the MC estimate of (\ref{eq:stoch_L_aux}) will suffer from prohibitively high variance...

variance reduction ... reparam ... A but we also note that the additional lower bounding step (\ref{eq:lowerlower}) allows the resulting auxiliary lower bound (\ref{eq:L_aux}) to be written as

We can rearrange (\ref{eq:L_aux}) as follows:
%
\begin{align}
\mathcal{L}_{aux} &= \Ex_{q(Z)}[ \log p(Y|Z) ] \notag \\
&- \Ex_{q(\fb,U,X)}[ \KL[q(Z|\fb,U,X)\Vert p(Z)] ]\notag\\
&- \Ex_{q(Z)}[ \KL[q(\fb,U,X)\Vert r(\fb,U,X|Z)] ].\label{eq:L_aux_ABC}
\end{align}
%
As noted by Kingma and Welling, in the case of Gaussian $p(Z)$ and $q(Z|\fb,U,X)$, the first K-L divergence can be evaluated analytically and, but the lower lower means we can get a second analytical term...
This was also noticed by Tran et al., in their lower lower bound...

Variational compression ... sparse variational GPs ... set $r(\fb|U,X,Z):=q(\fb|U,X)$ which means that $r(\fb,U,X|Z) = q(\fb|U,X)r(U,X|Z)$.
Full implications of this??? Reduces configuration space...
The aptness of this hinges on the veracity of the assumption that ... are sufficient statistics for ... in expectation under ...

Amortised inference ... VAE, back-constraints, ... we test several options: MLP vs kernel, q(X) vs r(U,X)...

\subsection{Design Choices}\label{subsec:choices}

The alternative configurations...

\section{Related Work}\label{sec:related}

Coupling mean field factors: Tran's Copula approach, the other copula approach ??, Michael Jordan paper.

The independent works of Kingma and Welling and Rezende et al. popularised the modern approach of autoencoding in the variational framework, and spurred on a great research drive in variational approaches to deep learning.
Highly sophisticated models such as DRAW - a method to.... - have at their core the VAE... Grosse et al considered an alternative variational lower bound based on importance weights to train a VAE.
However in all these approaches, the

deep gp - used autoencoding as backconstraints to make it scale (finite number of parameters)

Salimans and Welling considered a Markov chain inference network in which the final ($T^th$) Markov transition in the chain produced the latent variable (sample), such that previous $T-1$ variables were treated as auxiliary variables.
Collapsing over this expanded auxiliary space then resulted in a rich yet intractable mixture... and tractability was recovered through the use of an auxiliary distribution also of a Markov structure.

Most similar in spirit to GP-LVM-CMF approach is that of the \emph{Variational Gaussian Process} developed independently by Tran et al., which was also used conditional GPs (conditioned on aux params) in the inference model to couple the factors.
The key difference is that each latent variable is encoded with a \emph{separate} conditional GP... and the coupling between the latent variables was induced by evaluating each conditional GP at the same input location.
This leads to coupling whereby each GP tends to be positive in similar regions ?????.
In our approach the coupling is induced via the shared inducing coordinates... more faithfully to the spirit of the GP-LVM.

\section{Experimental Setup}\label{sec:experiments}

... free form q(X) means having to optimise the auxiliary latent coordinate of the test location just to be able to evaluate.
Including an optimisation procedure in the predictive step is an interesting idea, but was avoided here for lack of space and clarity of discussion, so investigation was deferred to a future study.

\section{Discussion and Future Work}\label{sec:discussion}

\section{TO DO:}
\begin{itemize}
  \item Graphs for the 3 models.
  \item Algorithm in section \ref{sec:gplvmcmf}
  \item Discussion of the aux variables acting as a stochastic hidden layer which provides smoothing...
  \item Moreover, the additional KL penalisation between the variational posterior and the auxiliary distribution prevents the variational model trying to fit directly to the true posterior and instead does so via fitting its denominator (the var posterior) to the aux dist. The aux params are generated from both the latent variables and the data, acting as an intermediate between the generative likelihood model and the marginal variational model.
  \item Links from above to batch normalisation - the auxiliary variables act like pseudo data which are global (not batch-specific).
\end{itemize}

\section{NOTES}

From another perspective, the partial feedback of of z to the aux params (captured in r) means that there is a term in the cost function (aux lower bound) which is a direct effect of the generative power of aux params to generate z - an indirect is achieved when z is first passed through the generative model. So direct feedback is a reinforce step - it reinforces our current belief over z.
It endows the scheme with a memory!

The above is for the aux lower lower bound in general, but we have even more motivation to use it in the case of the GP-LVM-CMF: want to avoid having to do optimization to make a prediction of a new data point!

change!!!!@

\bibliography{paper.bib}{}
\bibliographystyle{plainnat}


\end{document}
